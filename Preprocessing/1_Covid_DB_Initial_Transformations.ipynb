{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import urllib\n",
    "import sqlalchemy\n",
    "from shutil import move, copytree, copy2\n",
    "from datetime import datetime\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "\n",
    "def hash_md5(file_to_hash):\n",
    "    \"\"\"Takes a list of full paths and returns a list of MD5 hashes\"\"\"\n",
    "    file_md5 = []\n",
    "    if type(file_to_hash) is str:\n",
    "        file_to_hash = [file_to_hash]\n",
    "\n",
    "    BLOCK_SIZE = 65536*10 # The size of each read from the file\n",
    "\n",
    "    for i, file_name in progress_bar(enumerate(file_to_hash),total=len(file_to_hash)):\n",
    "        file_hash = hashlib.md5() # Create the hash object, can use something other than `.sha256()` if you wish\n",
    "        with open(file_name, 'rb') as f: # Open the file to read it's bytes\n",
    "            fb = f.read(BLOCK_SIZE) # Read from the file. Take in the amount declared above\n",
    "            while len(fb) > 0: # While there is still data being read from the file\n",
    "                file_hash.update(fb) # Update the hash\n",
    "                fb = f.read(BLOCK_SIZE) # Read the next block from the file\n",
    "        file_md5.append(file_hash.hexdigest())\n",
    "    return file_md5\n",
    "\n",
    "def connect_sql(server = '*****', database = '*****'):\n",
    "    username = pd.read_csv('*****',\"|\", header=None).iloc[0][0]\n",
    "    password = pd.read_csv('*****',\"|\", header=None).iloc[0][1]\n",
    "    params = urllib.parse.quote_plus('DRIVER={ODBC Driver 17 for SQL Server};SERVER=tcp:'+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "    engine = sqlalchemy.create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params)\n",
    "    print(f'Connected to {database}')\n",
    "    return engine\n",
    "\n",
    "def upload_sql(df_to_upload, chunksize, table_name, schema):\n",
    "    for i in progress_bar(range(len(df_to_upload)//chunksize+1),total=(len(df_to_upload)//chunksize+1)):\n",
    "        df_to_upload[i*chunksize:(i+1)*chunksize].to_sql(table_name, engine, schema=schema, if_exists=\"append\", index = False, method = \"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_to_import = '*****'\n",
    "\n",
    "resolution = 224\n",
    "working_directory = '*****'\n",
    "\n",
    "folder_to_import = Path(folder_to_import)\n",
    "working_directory = Path(working_directory)\n",
    "dicom_path = Path(os.path.join(working_directory, 'dicom'))\n",
    "image_path = Path(os.path.join(working_directory, 'jpg', str(resolution)))\n",
    "image_processing_path = Path(os.path.join(working_directory, 'jpg', 'being_processed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = connect_sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) DICOM Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = 'SELECT **** )'\n",
    "\n",
    "pd_dicom_paths = pd.read_sql_query(statement, con=engine)\n",
    "pd_dicom_paths[\"filename\"] = pd_dicom_paths.source_path.str.split('/').str[-1]\n",
    "\n",
    "print(f'Rows loaded: {pd_dicom_paths.shape[0]}')\n",
    "\n",
    "pd_dicom_paths = pd_dicom_paths[~pd_dicom_paths.duplicated(subset='path', keep='first')].reset_index(drop=True)\n",
    "print(f'Rows to insert after having removing duplicates: {pd_dicom_paths.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Columns to extract from DICOM files\n",
    "dcm_fields = ['AquisitionDate',\n",
    " 'AcquisitionDeviceProcessingDescription',\n",
    " 'BodyPartExamined',\n",
    " 'BurnedInAnnotation',\n",
    " 'DetectorManufacturerName',\n",
    " 'DetectorManufacturerModelName',\n",
    " 'DeviceSerialNumber',\n",
    " 'Manufacturer',\n",
    " 'ManufacturerModelName',\n",
    " 'Modality',\n",
    " 'PerformedProcedureStepDescription',\n",
    " 'ProtocolName',\n",
    " 'RequestedProcedureDescription',\n",
    " 'StudyInstanceUID',\n",
    " 'StudyDate',\n",
    " 'StudyTime',\n",
    " 'ViewPosition',\n",
    " 'PhotometricInterpretation'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='93819' class='' max='93819' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [93819/93819 54:15<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=[]\n",
    "for p in progress_bar(pd_dicom_paths.source_path.tolist()):\n",
    "    dcm = pydicom.dcmread(str(p),stop_before_pixels=True)\n",
    "    datar = {f:dcm[f].value for f in dcm_fields if f in dcm }\n",
    "    data.append(datar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_data = pd.DataFrame(data)\n",
    "final_df = pd.concat([pd_dicom_paths.reset_index(drop=True), dicom_data.drop(columns={\"StudyInstanceUID\"})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We change \"path\" for its future location\n",
    "final_df[\"path\"] = str(dicom_path) + '/' + final_df.StudyDate.str[:4]+'_'+final_df.StudyDate.str[4:6]+'/'+final_df.StudyInstanceUID+'/'+final_df.SeriesInstanceUID+'/'+final_df.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'AcquisitionDeviceProcessingDescription' in final_df.columns:\n",
    "    final_df[\"AcquisitionDeviceProcessingDescription\"] = final_df[\"AcquisitionDeviceProcessingDescription\"].astype('str')\n",
    "\n",
    "final_df = final_df.replace('',np.NaN).replace('nan',np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 100\n",
    "\n",
    "upload_sql(final_df.drop(columns=['source_path', 'filename']), chunksize, '****', '****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"src_copy\"] = ''\n",
    "final_df[\"dst_copy\"] = ''\n",
    "\n",
    "for i in final_df.index:\n",
    "    final_df[\"src_copy\"][i] = '/'+os.path.join(*(final_df.source_path[i].split('/')[:-2]))\n",
    "    final_df[\"dst_copy\"][i] = '/'+os.path.join(*(final_df.path[i].split('/')[:-2]))\n",
    "\n",
    "to_copy = final_df[[\"src_copy\", \"dst_copy\"]].drop_duplicates().values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(to_copy)):\n",
    "    copytree(to_copy[i][0], to_copy[i][1], dirs_exist_ok=True)\n",
    "    if i%500 == 0:\n",
    "        print(f'Copied {i} files / {len(to_copy)}')\n",
    "        if i > 0:\n",
    "            duration = (datetime.now() - start).total_seconds()\n",
    "            print(f'Speed: {500*60*60/duration} folders/h')\n",
    "        start = datetime.now()\n",
    "        \n",
    "print(f'Copied {i+1} folders / {len(to_copy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) JPG EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll just select only Dicoms that are NOT in the folder_results as jpg, just to increase speed and allow resume of job\n",
    "# We remove all Dicoms that don't have property \"PhotometricInterpretation\", since they'll return errors.\n",
    "\n",
    "df = pd.read_sql_query(\"****\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_outside(img,mask):\n",
    "    m,n = mask.shape\n",
    "    mask0,mask1 = mask.any(0),mask.any(1)\n",
    "    col_start,col_end = mask0.argmax(),n-mask0[::-1].argmax()\n",
    "    row_start,row_end = mask1.argmax(),m-mask1[::-1].argmax()\n",
    "    return img[row_start:row_end,col_start:col_end]\n",
    "\n",
    "def open_dcm(row):\n",
    "    dcm = pydicom.dcmread(row.path)\n",
    "    w = dcm.WindowWidth if 'WindowWidth' in dcm else 4000\n",
    "    c = dcm.WindowCenter if 'WindowCenter' in dcm else 2000\n",
    "    i = dcm.RescaleIntercept if 'RescaleIntercept' in dcm else 0\n",
    "    s = dcm.RescaleSlope if 'RescaleSlope' in dcm else 1\n",
    "    phi = dcm.PhotometricInterpretation\n",
    "\n",
    "    #remove black border surrounding image\n",
    "    arr = dcm.pixel_array\n",
    "    amx, amn = arr.max(),arr.min()\n",
    "    tolerance = 30 #hounsfield units to exclude black border\n",
    "    if abs(c+w/2-amx)>100: w,c = amx-amn, (amx+amn)/2 #sometimes w,c are wrong\n",
    "    if phi == 'MONOCHROME2':\n",
    "        arr = crop_outside(arr,arr>amn+tolerance)\n",
    "        arr = (c-w/2+arr)/w*255\n",
    "    elif phi== 'MONOCHROME1' :\n",
    "        arr = crop_outside(arr,arr<amx-tolerance)\n",
    "        arr = (c+w/2-arr)/w*255\n",
    "    else:\n",
    "        raise Exception('Image must be monochrome')             \n",
    "    arr = (arr+i)/s\n",
    "    return PIL.Image.fromarray(arr.astype(np.ubyte)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(image_processing_path, exist_ok=True)\n",
    "\n",
    "size = (resolution,resolution)\n",
    "\n",
    "def convert_img(_,idx):\n",
    "    row = df.iloc[idx]\n",
    "    img = open_dcm(row)\n",
    "    img= img.resize(size, PIL.Image.LANCZOS)\n",
    "    image_name = str(row.path.split('/')[-1].strip('.dcm'))+'.jpg' \n",
    "    img.save(image_processing_path / image_name)\n",
    "    \n",
    "parallel(convert_img, df.index)\n",
    "print(f'Done converting {len(df)} dicoms to {size} size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_paths = pd.DataFrame({'path':list(image_processing_path.rglob(\"*.jpg\"))})\n",
    "print(\"List of converted jpgs read\")\n",
    "jpg_paths[\"SOPInstanceUID\"] = jpg_paths.path.astype(str).str.split('/').str[-1].str[:-4]\n",
    "\n",
    "converted_jpgs = df[df.SOPInstanceUID.isin(jpg_paths.SOPInstanceUID)]\n",
    "\n",
    "if len(converted_jpgs) > 0:\n",
    "    chunksize = 1000\n",
    "\n",
    "    engine.execute('DROP TABLE IF EXISTS ****')\n",
    "\n",
    "    upload_sql(converted_jpgs.SOPInstanceUID, chunksize, '****', '****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(converted_jpgs) > 0:\n",
    "    statement = 'UPDATE *****'\n",
    "    engine.execute(statement)\n",
    "    statement = 'UPDATE *****'\n",
    "    engine.execute(statement)\n",
    "    statement = 'DROP TABLE ****'\n",
    "    engine.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in converted_jpgs.iterrows():\n",
    "    os.makedirs(os.path.join(image_path, row['StudyMonth']), exist_ok=True)\n",
    "    move(os.path.join(image_processing_path, row['SOPInstanceUID']+'.jpg'), os.path.join(image_path, row['StudyMonth'], row['SOPInstanceUID']+'.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) HASH FUNCTION (DUPLICATE CHECK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select all images that don't have MD5 calculated\n",
    "\n",
    "md5_calculation = pd.read_sql_query(\"SELECT *****\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_md5 = hash_md5(md5_calculation.path_jpg)\n",
    "\n",
    "md5_calculated = pd.DataFrame(list(zip(md5_calculation.SOPInstanceUID, file_md5)), columns =['SOPInstanceUID', 'MD5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(md5_calculated) > 0:\n",
    "    chunksize = 1000\n",
    "\n",
    "    engine.execute('DROP TABLE IF EXISTS ****')\n",
    "    \n",
    "    upload_sql(md5_calculated, chunksize, '****', '*****')\n",
    "\n",
    "    statement = 'UPDATE ****'\n",
    "    engine.execute(statement)\n",
    "    statement = 'DROP TABLE ****'\n",
    "    engine.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIMDCAT-AQuAS - Marc",
   "language": "python",
   "name": "marc-simdcat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
